# -*- coding: utf-8 -*-
"""Student Score Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z5T5PSBD-5DfHEyvJUo8c1WVIG0sH9t2

# Project – Student Score Prediction

The goal of this project is to build a model that can predict student exam scores based mainly on the number of hours they studied. The dataset has different factors related to student life, but for this project the focus was kept on the link between study time and exam results.

To do this, a **Linear Regression model** was used. This type of model tries to draw a straight line that best fits the data. The slope of the line shows how much the score is likely to increase for each extra hour of study, while the intercept gives the score when study hours are zero.

The dataset was divided into a training set and a testing set. The training data was used to fit the model, and the testing data was used to check how well it works on new examples. The accuracy of the model was measured using MAE, RMSE, and R² score.

The results showed a clear positive relationship: students who studied more hours usually scored higher. The regression line matched the data trend well, which means linear regression was a good choice here.

In conclusion, the project shows that study hours have a strong effect on exam performance. While there are other factors that can also influence results, this simple model highlights the importance of consistent study time in achieving better scores.
"""

!pip install kaggle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d yassaanwer/student-performance-factors
!unzip -o student-performance-factors.zip -d student_data

df = pd.read_csv("student_data/StudentPerformanceFactors.csv")
df.head()

df.info()
print(df.describe(include="all"))
print(df.isnull().sum())

df = df.drop_duplicates()
df = df.dropna()

print("Dataset shape after cleaning:", df.shape)
df.head()

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
def _norm(col):
    return re.sub(r'[^a-z0-9]+', '', str(col).strip().lower())

norm_map = {_norm(c): c for c in df.columns}
feature_keys = [
    "hoursstudied", "studyhours", "studyhour", "studytime", "studytimehours",
    "studyduration", "hoursofstudying", "hourstudied"
]
target_keys = [
    "marks", "examscore", "finalscore", "score", "scores", "g3",
    "finalgrade", "grade", "performanceindex", "percentage", "result",
    "finalmarks", "exammarks", "totalscore"
]

feature_col = next((norm_map[k] for k in feature_keys if k in norm_map), None)
target_col  = next((norm_map[k] for k in target_keys  if k in norm_map), None)

if feature_col is None or target_col is None:
    print("❌ Could not auto-detect columns.\nAvailable columns:", list(df.columns))
    raise ValueError(
        "Need a study-hours column (e.g., 'Hours_Studied', 'Study_Hours') "
        "and a target score column (e.g., 'Exam_Score', 'Final_Score', 'Marks')."
    )

print(f"✅ Using feature (study hours): {feature_col}")
print(f"✅ Using target (score):       {target_col}")
work = df[[feature_col, target_col]].copy()
work[feature_col] = pd.to_numeric(work[feature_col], errors='coerce')
work[target_col]  = pd.to_numeric(work[target_col],  errors='coerce')
work = work.dropna()

if work.empty:
    raise ValueError(
        f"After numeric conversion there are no valid rows for '{feature_col}' and '{target_col}'. "
        "Please check your CSV values."
    )
df_numeric = df.apply(pd.to_numeric, errors='coerce')  # coerce numeric-like strings
corr = df_numeric.corr(numeric_only=True)

if corr.size > 0 and not corr.isna().all().all():
    plt.figure(figsize=(8,6))
    sns.heatmap(corr, annot=True, cmap="coolwarm")
    plt.title("Correlation Heatmap (Numeric Features Only)")
    plt.show()
else:
    print("ℹ️ Skipping heatmap: no numeric columns detected.")
plt.figure(figsize=(7,5))
plt.scatter(work[feature_col], work[target_col], alpha=0.7)
plt.xlabel(feature_col)
plt.ylabel(target_col)
plt.title(f"{feature_col} vs {target_col}")
plt.show()
X = work[[feature_col]]  # 2D
y = work[target_col]     # 1D
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("Train shape:", X_train.shape, "Test shape:", X_test.shape)
model = LinearRegression()
model.fit(X_train, y_train)

print("Coefficient (slope):", model.coef_[0])
print("Intercept:", model.intercept_)
y_pred = model.predict(X_test)
comparison = pd.DataFrame({ "Actual": y_test.values, "Predicted": y_pred })
print("\nSample predictions:")
display(comparison.head())
mae  = mean_absolute_error(y_test, y_pred)
mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2   = r2_score(y_test, y_pred)

print(f"\nMAE:  {mae:.3f}")
print(f"MSE:  {mse:.3f}")
print(f"RMSE: {rmse:.3f}")
print(f"R²:   {r2:.3f}")
plt.figure(figsize=(7,5))
plt.scatter(X_test[feature_col], y_test, label="Actual", alpha=0.7)
order = np.argsort(X_test[feature_col].values)
plt.plot(
    X_test[feature_col].values[order],
    y_pred[order],
    linewidth=2,
    label="Regression Line"
)
plt.xlabel(feature_col)
plt.ylabel(target_col)
plt.title("Actual vs Predicted (Test Set)")
plt.legend()
plt.show()

X = work[[feature_col]]
y = work[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

model = LinearRegression()
model.fit(X_train, y_train)

print("Coefficient (slope):", model.coef_[0])
print("Intercept:", model.intercept_)

y_pred = model.predict(X_test)

comparison = pd.DataFrame({
    "Actual": y_test.values,
    "Predicted": y_pred
})
comparison.head()

mae  = mean_absolute_error(y_test, y_pred)
mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2   = r2_score(y_test, y_pred)

print(f"MAE:  {mae:.2f}")
print(f"MSE:  {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R²:   {r2:.2f}")

plt.figure(figsize=(7,5))
plt.scatter(X_test[feature_col], y_test, color="blue", label="Actual")
plt.plot(X_test[feature_col], y_pred, color="red", linewidth=2, label="Regression Line")
plt.xlabel(feature_col)
plt.ylabel(target_col)
plt.title("Actual vs Predicted Scores")
plt.legend()
plt.show()