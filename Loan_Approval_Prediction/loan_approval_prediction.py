# -*- coding: utf-8 -*-
"""Loan Approval Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlc5vf5BUcEU5A8A9eoJ3rBIdUhrw5Zd
"""

!pip install imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

from imblearn.over_sampling import SMOTE

np.random.seed(42)
n_samples = 1000

data = {
    "Gender": np.random.choice(["Male", "Female"], size=n_samples),
    "Married": np.random.choice(["Yes", "No"], size=n_samples),
    "Dependents": np.random.choice(["0", "1", "2", "3+"], size=n_samples),
    "Education": np.random.choice(["Graduate", "Not Graduate"], size=n_samples),
    "Self_Employed": np.random.choice(["Yes", "No"], size=n_samples),
    "ApplicantIncome": np.random.randint(1500, 25000, size=n_samples),
    "CoapplicantIncome": np.random.randint(0, 10000, size=n_samples),
    "LoanAmount": np.random.randint(50, 700, size=n_samples),
    "Loan_Amount_Term": np.random.choice([360, 120, 180, 240], size=n_samples),
    "Credit_History": np.random.choice([1.0, 0.0], size=n_samples, p=[0.8, 0.2]),
    "Property_Area": np.random.choice(["Urban", "Rural", "Semiurban"], size=n_samples),
    "Loan_Status": np.random.choice(["Y", "N"], size=n_samples, p=[0.7, 0.3])
}

df = pd.DataFrame(data)
for col in ["Gender", "Married", "Dependents", "Self_Employed", "LoanAmount"]:
    df.loc[df.sample(frac=0.05).index, col] = np.nan

df.head()

print(df.info())
print("\nMissing values:\n", df.isnull().sum())

df["Gender"].fillna(df["Gender"].mode()[0], inplace=True)
df["Married"].fillna(df["Married"].mode()[0], inplace=True)
df["Dependents"].fillna(df["Dependents"].mode()[0], inplace=True)
df["Self_Employed"].fillna(df["Self_Employed"].mode()[0], inplace=True)
df["LoanAmount"].fillna(df["LoanAmount"].median(), inplace=True)

print("Missing values after imputation:\n", df.isnull().sum())

categorical_cols = ["Gender", "Married", "Dependents", "Education", "Self_Employed", "Property_Area", "Loan_Status"]

encoder = LabelEncoder()
for col in categorical_cols:
    df[col] = encoder.fit_transform(df[col])

df.head()

X = df.drop("Loan_Status", axis=1)
y = df["Loan_Status"]

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

X.head()

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)

print("Before SMOTE:", y.value_counts())
print("After SMOTE:", y_res.value_counts())

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)

models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"\n===== {name} =====")
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

"""#Loan Approval Prediction

This project was about predicting whether someone applying for a loan is going to get it approved or not. At first glance, it sounds simple — yes or no — but it turned out to be a bit more involved than that. The main challenge was that the data wasn’t balanced. There were way more cases of approved loans compared to denied ones, which means a model could easily just say “approved” every time and still look pretty accurate. That wouldn’t be very useful in practice, so I knew from the start I had to look at other evaluation measures like precision, recall, and the F1 score instead of just accuracy.

The dataset had a mix of numbers and categories. Things like income and loan amount were straightforward, while details like gender, marital status, or property area were text-based. Some values were missing here and there, which is always a bit annoying but expected. I filled them in — using the most common values for categorical fields and the median for numbers. I figured that would be the least disruptive way. After that, I needed to convert all the categorical features into numbers. Label encoding did the job, though I did think about one-hot encoding for a second, but it would have blown up the number of columns, so I skipped it.
  
Something I didn’t realize until I started looking at the distributions was how different the scales were. Income values, for example, were way higher compared to credit history, so I standardized everything. Otherwise, the models might have paid more attention to income just because of its scale, not because it’s actually more important. On top of that, I used SMOTE to handle the imbalance, which basically created synthetic examples for the underrepresented class (the denied loans). That helped even things out so the models wouldn’t ignore that category.
  
For the modeling part, I tried a few approaches. Logistic regression was the starting point since it’s quick to run and gives a decent baseline. Then I went for decision trees and random forests because they can capture more complex patterns. I also included an SVM to see if it would perform well on the separation. To evaluate them, I checked precision, recall, F1 scores, and confusion matrices.
  
The results were kind of what I expected but with some surprises. Random forest came out on top, giving a good balance between precision and recall. Logistic regression was easy to interpret but missed out on some recall, so it wasn’t catching all the denied loans. The SVM worked but needed more tuning than I wanted to spend time on, and the decision tree did okay but clearly leaned toward overfitting. At first, I actually thought logistic regression would hold up better, but after running everything, random forest was clearly stronger.
  
  The main takeaway for me was that this kind of prediction task isn’t just about chasing the highest accuracy. It’s more about finding the right trade-off because approving someone who shouldn’t get a loan or denying someone who should both have real-world consequences. That’s why focusing on recall and F1 score made more sense than just bragging about accuracy. Overall, this project gave me a good reminder that preparing the data and choosing the right metrics are just as important as the actual model building.
"""